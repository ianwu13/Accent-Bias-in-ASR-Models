# Accent-Bias-in-ASR-Models

TODO: GENERAL DESCRIPTION

***

## Data Processing
Data preprocessing scripts are contained in the `data` directory. This directory should also be used to store the actual dataset. Scripts use globals for pathing, as the directory structure is assumed to be static. These can easily be altered to apply these scripts to alternative data/directory structures, however.

### Accent mappings
A mapping for accent lables present in the original data to accent groups on which we perform our comparative analysis is stored as a json object in `accents.json`. This mapping was generated by us, based on literature review and the samples in the dataset

### `download_data.py`
A variation of `download_data_ref_file.py` which only downloads audio samples for valid rows. This script does not reference a valid samples files, but may be preferable if computational resources are limited.

### `generate_agglomarative_samples.py`
Generates multi-accent samples from already downloaded single-accent samples. A tsv file of samples if provided, along with the common sample rate to convert samples to (usually 16000 Hz). Samples are then saved and recorded in another tsv file.

### `preprocess_tabular_data.py`
Preprocesses the original tsv file containing all dataset samples (excluding actual audio files). This process includes filtering rows without valid accent labels, mapping accent labels to groups, and text preprocessing for the sentence spoken in the recording.

### `filter_for_accent.py`
This script is redundant to `preprocess_tabular_data.py`, which also performs filtering. However, this script was used to help facilitate the design of our experimental setup and for determining accent groups to be used.

### `download_data_ref_file.py`
Downloads audio files only for valid samples (those with selected accent labels), which are stored in a reference file. Valid samples are provided from a .tsv file. Samples is retrieved from the HuggingFace Hub, then cross referenced with the file before being saved as a .wav file.

***

## Transcription
3 scripts are provied to transcribe audio samples with different ASR models. These scripts accept similar arguments, briefly described below, but `python3 <SCRIPT>.py --help` should be called for more detailed descriptions.

### `transcribe_cv16_whisper.py`
Transcribes the dataset specified with the "--dataset" argument using OpenAI's Whisper model. Different variations of the Whisper model (e.g., large, base, base.en, etc.) can be specified with the "--model" argument.

### `transcribe_cv16_wav2vec2.py`
Performs transcription using Facebook/Meta's's Wav2Vec2 model. Different variations of the Wav2Vec2 model (e.g., large, base, etc.) can be specified with the "--model" argument.

### `transcribe_cv16_canary.py`
This Script is used to run transcription of the dataset using Nvidia's Canary-1B Model. This model required the use of the NeMo Package to be run, which can be installed along with its dependencies suing the following commands:
```
pip install git+https://github.com/NVIDIA/NeMo.git
pip install hydra-core pytorch-lightning lhotse jiwer pyannote.audio webdataset datasets
```

***

## Evaluation

### `evaluate_single_accent.py`
TODO

### `evaluate_multi_accent.py`
TODO
